cmake_minimum_required(VERSION 3.5)
project(liteqwen LANGUAGES CXX CUDA)
enable_language(CUDA)
set(CUDA_VERBOSE_BUILD ON)

# cuda flags from xformers
set(CUDA_FLAGS
  -DHAS_PYTORCH
  -U__CUDA_NO_HALF_OPERATORS__
  -U__CUDA_NO_HALF_CONVERSIONS__
  --expt-extended-lambda
  --expt-relaxed-constexpr
  --use_fast_math
  -D_ENABLE_EXTENDED_ALIGNED_STORAGE
  --ptxas-options=-v
  --threads
  4
  --ptxas-options=-O2 # for cuda > 11.6
  --ptxas-options=-allow-expensive-optimizations=true # for cuda > 11.6
)

find_package(CUDA  REQUIRED)
include_directories("${CUDA_INCLUDE_DIRS}")

# lookup for pytorch and include its library
execute_process(
    COMMAND
      python -c
      "import torch; import os; print(os.path.dirname(torch.__file__), end='')"
    OUTPUT_VARIABLE TORCH_PATH)
  list(APPEND CMAKE_PREFIX_PATH ${TORCH_PATH})
message(TORCH_PATH= ${TORCH_PATH})

set(TORCH_INCLUDE ${TORCH_PATH}/include)
include_directories("${TORCH_INCLUDE}")
set(TORCH_LINKED_LIB ${TORCH_PATH}/lib)

set(CMAKE_BUILD_TYPE "Release")
if (CMAKE_CXX_COMPILER_ID STREQUAL "Clang")
    set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -pthread --std=c++17 -O2")
elseif(CMAKE_CXX_COMPILER_ID STREQUAL "MSVC")
    set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -DNOMINMAX -O2 /std:c++17 /arch:AVX /source-charset:utf-8")
else()
    set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -pthread --std=c++17 -O2 -march=native")
endif()

message(STATUS "CMAKE_CXX_FLAGS " ${CMAKE_CXX_FLAGS})
message(STATUS "CUDA_FLAGS" ${CUDA_FLAGS})

# =============== pack implementing test codes, should be ignored for library build============
message(STATUS "CMAKE_CURRENT_SOURCE_DIR=" ${CMAKE_CURRENT_SOURCE_DIR})
include_directories(src/include)
include_directories(src/include/device)
include_directories(src/cutlass_fmha)
include_directories(src/exllama/cuda_func)
include_directories(src/exllama)

set(LITEQWEN_CXX_SOURCES src/core_cpu.cpp src/json11.cpp src/kv_cache.cpp src/pool.cpp src/entities.cpp)
set(LITEQWEN_CUDA_SOURCES src/core_gpu.cu src/forward_gpu.cu src/sampling.cu)

set(LITEQWEN_LINKED_LIBS ${LITEQWEN_LINKED_LIBS} cublas)

message(STATUS "LITEQWEN_LINKED_LIBS " ${LITEQWEN_LINKED_LIBS})
set(CMAKE_CUDA_ARCHITECTURES "native")

file(GLOB XFORMER_AUTOGEN_IMPL
     "src/cutlass_fmha/autogen/impl/cutlassF_f*.cu" "src/cutlass_fmha/autogen/impl/cutlassF_b*.cu"
)
message(STATUS "XFORMER_AUTOGEN_IMPL" ${XFORMER_AUTOGEN_IMPL})
set(LITEQWEN_ATTN_SOURCES src/cutlass_fmha/xformer_attention.cu ${XFORMER_AUTOGEN_IMPL})

# compile xllama for 4bit quantization
file(GLOB EXLLAMA_IMPL "src/exllama/cuda_func/q4_matrix.cu" "src/exllama/cuda_func/q4_matmul.cu" "src/exllama/cuda_func/column_remap.cu" "src/exllama/cuda_buffers.cu")
set(LITEQWEN_QUANT_SOURCES src/exllama/exllama_liteqwen_ext.cpp ${EXLLAMA_IMPL})

add_library(liteqwen_conn SHARED     
    ${LITEQWEN_CXX_SOURCES}
    ${LITEQWEN_CUDA_SOURCES}
    ${LITEQWEN_ATTN_SOURCES}
    ${LITEQWEN_QUANT_SOURCES}
)
target_compile_features(liteqwen_conn PUBLIC cuda_std_17)
target_link_libraries(liteqwen_conn PUBLIC ${LITEQWEN_LINKED_LIBS} ${TORCH_LINKED_LIB})

target_include_directories(liteqwen_conn PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/src/include ${CMAKE_CURRENT_SOURCE_DIR}/src/include/device)
target_include_directories(liteqwen_conn PUBLIC third-party/cutlass/include third-party/cutlass/examples)
# target_include_directories(liteqwen_conn PUBLIC src/exllama)

target_compile_options(liteqwen_conn PUBLIC
    # $<$<COMPILE_LANGUAGE:CXX>:${CXX_FLAGS}>
    $<$<COMPILE_LANGUAGE:CUDA>:${CUDA_FLAGS}>
)

# # ----adding test executable -----
# target_include_directories(liteqwen_conn PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/src/include ${CMAKE_CURRENT_SOURCE_DIR}/src/include/device)

link_directories("/usr/local/cuda/lib64")
add_executable(
    test_main src/test_main.cpp 
)
target_compile_features(test_main PUBLIC cuda_std_17)
# target_include_directories(test_main PUBLIC third-party/cutlass/include third-party/cutlass/examples)
target_link_libraries(test_main PUBLIC liteqwen_conn)
